{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"34b59eca-11cb-489b-a29b-7b755f4f9f13"},"source":"from __future__ import absolute_import, division, print_function, unicode_literals\r\nimport numpy as np\r\nimport tensorflow\r\nimport pandas as pd\r\nfrom tensorflow.keras import optimizers\r\nfrom tensorflow import squeeze\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\nfrom tensorflow.keras.preprocessing.text import Tokenizer\r\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\r\nfrom tensorflow.keras.layers import BatchNormalization\r\nfrom tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout, Input, Flatten\r\nfrom tensorflow.keras.models import Sequential\r\nimport matplotlib.pyplot as plt\r\n\r\n","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"6ec7b3ef-9344-4f5f-a9af-cff3027b3162"},"source":"train = pd.read_csv('../preprocessing_scripts/train.csv')\r\ntest = pd.read_csv('../preprocessing_scripts/test.csv')","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"472aebf7-7983-46c0-84b9-ec275397a08f"},"source":"import bert\r\nmodel_name = \"uncased_L-12_H-768_A-12\"\r\nmodel_dir = bert.fetch_google_bert_model(model_name, \".models\")\r\nmodel_ckpt = os.path.join(model_dir, \"bert_model.ckpt\")\r\n\r\nbert_params = bert.params_from_pretrained_ckpt(model_dir)\r\nl_bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\",trainable=False)\r\nmax_seq_len = 100\r\nl_input_ids      = tensorflow.keras.layers.Input(shape=(max_seq_len,), dtype='int32')\r\nl_token_type_ids = tensorflow.keras.layers.Input(shape=(max_seq_len,), dtype='int32')\r\n\r\n# using the default token_type/segment id 0\r\nbertLayer = l_bert(l_input_ids)\r\n#flat = Flatten()(bertLayer)\r\nflat = Bidirectional(GRU(64, return_sequences=True))(bertLayer)\r\nflat = Bidirectional(GRU(64, return_sequences=True))(flat)\r\nflat = Bidirectional(GRU(64, return_sequences=True))(flat)\r\nflat = Bidirectional(GRU(64, return_sequences=True))(flat)\r\nflat = Bidirectional(GRU(64))(flat)\r\nflat = Dense(128, activation='tanh')(flat)\r\n\r\noutput = Dense(1,activation = 'sigmoid')(flat)\r\n                         # output: [batch_size, max_seq_len, hidden_size]\r\nmodel = tensorflow.keras.Model(inputs=l_input_ids, outputs=output)\r\nmodel.build(input_shape=(None, max_seq_len))\r\nbert.load_bert_weights(l_bert, model_ckpt) ","outputs":[{"name":"stdout","text":"Already  fetched:  uncased_L-12_H-768_A-12.zip\nalready unpacked at: .models/uncased_L-12_H-768_A-12\nDone loading 196 BERT weights from: .models/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7ff3ca2f4128> (prefix:bert_2). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\nUnused weights from checkpoint: \n\tbert/embeddings/token_type_embeddings\n\tbert/pooler/dense/bias\n\tbert/pooler/dense/kernel\n\tcls/predictions/output_bias\n\tcls/predictions/transform/LayerNorm/beta\n\tcls/predictions/transform/LayerNorm/gamma\n\tcls/predictions/transform/dense/bias\n\tcls/predictions/transform/dense/kernel\n\tcls/seq_relationship/output_bias\n\tcls/seq_relationship/output_weights\n","output_type":"stream","truncated":false},{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"[]"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"148be96a-6e55-4878-876f-14cf11f97590"},"source":"model.summary()","outputs":[{"name":"stdout","text":"Model: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_5 (InputLayer)         [(None, 100)]             0         \n_________________________________________________________________\nbert (BertModelLayer)        (None, 100, 768)          108890112 \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 100, 128)          320256    \n_________________________________________________________________\nbidirectional_2 (Bidirection (None, 100, 128)          74496     \n_________________________________________________________________\nbidirectional_3 (Bidirection (None, 100, 128)          74496     \n_________________________________________________________________\nbidirectional_4 (Bidirection (None, 100, 128)          74496     \n_________________________________________________________________\nbidirectional_5 (Bidirection (None, 128)               74496     \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 129       \n=================================================================\nTotal params: 109,524,993\nTrainable params: 634,881\nNon-trainable params: 108,890,112\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"df68ce59-634d-4bf8-9980-0e6afe630114"},"source":"with open('../preprocessing_scripts/new_train_data.txt', 'r') as f:\r\n  tweets = f.readlines()\r\n\r\nwith open('../preprocessing_scripts/targets.csv', 'r') as f:\r\n  targets = f.readlines()","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"aee8cd87-47d0-424c-812a-5e1808200c76"},"source":"max_words = 14000\r\ntokenizer = Tokenizer(num_words=max_words)\r\ntrainX = tweets[:6000]\r\ntrainY = targets[:6000]\r\ntestX = tweets[6000:]\r\ntestY = tweets[6000:]\r\nmaxlen = 100\r\ntokenizer.fit_on_texts(trainX)\r\ntokenized_version = tokenizer.texts_to_sequences(trainX)\r\ntokenized_version = pad_sequences(tokenized_version, maxlen=maxlen)","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"a3e48476-31b1-4a81-af06-b987943d6a79"},"source":"from tensorflow.keras import backend as K\ndef recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\ndef precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\ndef f1(y_true, y_pred):\n    precisionf = precision(y_true, y_pred)\n    recallf = recall(y_true, y_pred)\n    return 2*((precisionf*recallf)/(precisionf+recallf+K.epsilon()))","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"9ce08c83-9616-4f25-ad55-bdae8ffd50d5"},"source":"trainY = np.array(trainY,dtype = 'int32')\r\nmodel.compile(loss=\"binary_crossentropy\",\r\n              optimizer=\"adam\",\r\n              metrics=['accuracy',f1,recall,precision]) \r\n","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"2c71fdd6-528e-462b-a6e0-17e6d082ee03"},"source":"history = model.fit(x=tokenized_version, y=trainY, batch_size = 64, epochs=10, validation_split = 0.2)","outputs":[{"name":"stdout","text":"Train on 4800 samples, validate on 1200 samples\nEpoch 1/10\n4800/4800 [==============================] - 73s 15ms/sample - loss: 0.6884 - accuracy: 0.5708 - f1: 0.0392 - recall: 0.0535 - precision: 0.0451 - val_loss: 0.6851 - val_accuracy: 0.5742 - val_f1: 0.0000e+00 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\nEpoch 2/10\n4800/4800 [==============================] - 51s 11ms/sample - loss: 0.6845 - accuracy: 0.5702 - f1: 0.0431 - recall: 0.0469 - precision: 0.0537 - val_loss: 0.6883 - val_accuracy: 0.5767 - val_f1: 0.0321 - val_recall: 0.0174 - val_precision: 0.2368\nEpoch 3/10\n4800/4800 [==============================] - 51s 11ms/sample - loss: 0.6828 - accuracy: 0.5785 - f1: 0.0249 - recall: 0.0263 - precision: 0.0301 - val_loss: 0.6846 - val_accuracy: 0.5742 - val_f1: 0.0000e+00 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\nEpoch 4/10\n4800/4800 [==============================] - 51s 11ms/sample - loss: 0.6810 - accuracy: 0.5798 - f1: 0.0142 - recall: 0.0097 - precision: 0.0437 - val_loss: 0.6798 - val_accuracy: 0.5742 - val_f1: 0.0000e+00 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\nEpoch 5/10\n4800/4800 [==============================] - 51s 11ms/sample - loss: 0.6830 - accuracy: 0.5767 - f1: 0.0190 - recall: 0.0204 - precision: 0.0260 - val_loss: 0.6820 - val_accuracy: 0.5742 - val_f1: 0.0000e+00 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\nEpoch 6/10\n4800/4800 [==============================] - 51s 11ms/sample - loss: 0.6823 - accuracy: 0.5788 - f1: 0.0154 - recall: 0.0159 - precision: 0.0166 - val_loss: 0.6819 - val_accuracy: 0.5742 - val_f1: 0.0000e+00 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\nEpoch 7/10\n4800/4800 [==============================] - 51s 11ms/sample - loss: 0.6808 - accuracy: 0.5821 - f1: 0.0000e+00 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.6818 - val_accuracy: 0.5742 - val_f1: 0.0000e+00 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\nEpoch 8/10\n4800/4800 [==============================] - 51s 11ms/sample - loss: 0.6816 - accuracy: 0.5821 - f1: 0.0000e+00 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.6809 - val_accuracy: 0.5742 - val_f1: 0.0000e+00 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\nEpoch 9/10\n4800/4800 [==============================] - 51s 11ms/sample - loss: 0.6803 - accuracy: 0.5813 - f1: 0.0039 - recall: 0.0032 - precision: 0.0050 - val_loss: 0.6872 - val_accuracy: 0.5742 - val_f1: 0.0000e+00 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\nEpoch 10/10\n4800/4800 [==============================] - 51s 11ms/sample - loss: 0.6802 - accuracy: 0.5800 - f1: 0.0031 - recall: 0.0021 - precision: 0.0073 - val_loss: 0.6816 - val_accuracy: 0.5742 - val_f1: 0.0000e+00 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n","output_type":"stream","truncated":false}]}],"nbformat":4,"nbformat_minor":2,"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"4c9c087a-283f-448f-9a84-eaeba5a52278"}}