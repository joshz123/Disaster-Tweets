{"cells":[{"cell_type":"code","source":"!pip install nltk spacy","metadata":{"tags":[]},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/venv/lib/python3.7/site-packages (3.4.5)\nCollecting spacy\n  Using cached spacy-2.2.4-cp37-cp37m-manylinux1_x86_64.whl (10.6 MB)\nRequirement already satisfied: six in /opt/venv/lib/python3.7/site-packages (from nltk) (1.14.0)\nCollecting blis<0.5.0,>=0.4.0\n  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n\u001b[K     |████████████████████████████████| 3.7 MB 3.3 MB/s eta 0:00:01\n\u001b[?25hCollecting wasabi<1.1.0,>=0.4.0\n  Downloading wasabi-0.6.0-py3-none-any.whl (20 kB)\nCollecting plac<1.2.0,>=0.9.6\n  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/venv/lib/python3.7/site-packages (from spacy) (2.22.0)\nCollecting cymem<2.1.0,>=2.0.2\n  Downloading cymem-2.0.3-cp37-cp37m-manylinux1_x86_64.whl (32 kB)\nCollecting catalogue<1.1.0,>=0.0.7\n  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\nCollecting preshed<3.1.0,>=3.0.2\n  Downloading preshed-3.0.2-cp37-cp37m-manylinux1_x86_64.whl (118 kB)\n\u001b[K     |████████████████████████████████| 118 kB 48.6 MB/s eta 0:00:01\n\u001b[?25hCollecting srsly<1.1.0,>=1.0.2\n  Downloading srsly-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (185 kB)\n\u001b[K     |████████████████████████████████| 185 kB 65.0 MB/s eta 0:00:01\n\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n  Downloading murmurhash-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (19 kB)\nCollecting thinc==7.4.0\n  Downloading thinc-7.4.0-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n\u001b[K     |████████████████████████████████| 2.2 MB 49.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /opt/venv/lib/python3.7/site-packages (from spacy) (1.18.1)\nRequirement already satisfied: setuptools in /opt/venv/lib/python3.7/site-packages (from spacy) (45.1.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/venv/lib/python3.7/site-packages (from spacy) (4.43.0)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/venv/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (2.0.1)\nRequirement already satisfied: more-itertools in /opt/venv/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (8.1.0)\nInstalling collected packages: blis, wasabi, plac, cymem, catalogue, murmurhash, preshed, srsly, thinc, spacy\nSuccessfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.3 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 spacy-2.2.4 srsly-1.0.2 thinc-7.4.0 wasabi-0.6.0\n\u001b[33mWARNING: You are using pip version 20.0.1; however, version 20.0.2 is available.\nYou should consider upgrading via the '/opt/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream","truncated":false}]},{"cell_type":"code","metadata":{"tags":[]},"source":"import csv\r\nimport re\r\nimport pandas as pd\r\nimport nltk\r\nimport spacy\r\nfrom nltk.stem import WordNetLemmatizer\r\n\r\nnltk.download('wordnet')\r\nlemmatizer = WordNetLemmatizer()\r\nfilename_train = R\"test.csv\"\r\nnlp = spacy.load('en_core_web_sm')\r\n\r\ntargets = pd.read_csv(\"train.csv\")[\"target\"].to_csv(\"targets.csv\",header=None,index=None)\r\n\r\ndef is_ascii(s):\r\n    return all(ord(c) < 128 for c in s)\r\n\r\n\r\ndef extract_entities(s):\r\n    doc = nlp(s)\r\n    new_string = s\r\n    for ent in doc.ents:\r\n        new_string = new_string.replace(ent.text, ' ' + ent.label_ + ' ')\r\n    return new_string\r\n\r\n\r\ndef lemmatize_text(s):\r\n    new_string = ''\r\n    for word in s.split(' '):\r\n        try:\r\n            new_string += lemmatizer.lemmatize(word) + ' '\r\n        except:\r\n            new_string += word + ' '\r\n    return new_string\r\n\r\n\r\nkeywords = list()\r\nlocations = list()\r\ntexts = list()\r\ntargets = list()\r\nwith open(filename_train, encoding=\"utf8\") as csvfile:\r\n    reader = csv.DictReader(csvfile)\r\n    itr = 0\r\n    for row in reader:\r\n        keyword, location, text, target = row['keyword'], row['location'], row['text'], row['target']\r\n        keywords.append(keyword)\r\n        locations.append(location)\r\n        text = text.replace(\"\\n\", ' ')\r\n        text = text.replace(\"\\'\", ' ')\r\n        text = text.replace(\"\\\"\", ' ')\r\n        text = text.replace('\\n', ' ')\r\n        text = lemmatize_text(text)\r\n        texts.append(extract_entities(text))\r\n        targets.append(target)\r\n\r\nnew_texts = list()\r\nfor text in texts:\r\n    new_string = '<START> '\r\n    for token in text.split(' '):\r\n        if re.match(\"^((https|http|ftp|file)?:\\/\\/).*\", token):\r\n            new_string += \"<LINK> \"\r\n            continue\r\n        # need to check for time regex\r\n        if re.match(\"[0-9]+:[0-9]+(am|AM|pm|PM)?\", token):\r\n            new_string += \"TIME \"\r\n            continue\r\n\r\n        temp = token.replace(';', ' ')\r\n        temp = temp.replace('?', ' ')\r\n        temp = temp.replace(':', ' ')\r\n        temp = temp.replace('!', ' ')\r\n        if re.match(\"^-?\\d*\\.?\\d+$\", temp):\r\n            new_string += \"<NUM> \"\r\n            continue\r\n\r\n        temp = temp.replace('-', ' ')\r\n        temp = temp.replace('.', ' ')\r\n        temp = temp.replace('(', ' ')\r\n        temp = temp.replace(')', ' ')\r\n        temp = temp.replace('{', ' ')\r\n        temp = temp.replace('}', ' ')\r\n        temp = temp.replace('[', ' ')\r\n        temp = temp.replace(']', ' ')\r\n\r\n        if re.match(\"^@.*\", temp):\r\n            new_string += \"<USER>\"\r\n        elif re.match(\"^ @.*\", temp):\r\n            new_string += \"<USER>\"\r\n        elif re.match(\"^#.*\", temp):\r\n            new_string += \"<HASH>\"\r\n        elif not is_ascii(temp):\r\n            new_string += \"<NASCII>\"\r\n        else:\r\n\r\n            new_string += temp\r\n        new_string += ' '\r\n    new_string += \"<STOP>\"\r\n    new_string = new_string.replace('*', \" * \")\r\n    new_string = new_string.replace('&', ' ')\r\n    new_string = new_string.replace('$', ' ')\r\n    new_string = new_string.replace('#', ' ')\r\n    new_string = new_string.replace('^', ' ')\r\n    new_string = new_string.replace('\\\\', ' ')\r\n    new_string = new_string.replace('/', ' ')\r\n    new_string = new_string.replace('`', ' ')\r\n    new_string = new_string.replace('~', ' ')\r\n    new_texts.append(' '.join(new_string.split()))\r\nwith open(\"new_train_data.txt\", \"w\") as f:\r\n    for t in new_texts:\r\n        f.write(t + \"\\n\")\r\n\r\nprint(len(new_texts))\r\n","outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"output_type":"error","ename":"OSError","evalue":"[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-d6830d21774d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfilename_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mR\"test.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"targets.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."]}]}],"nbformat":4,"nbformat_minor":2,"metadata":{"deepnote_execution_queue":[]}}